# -*- coding: utf-8 -*-
"""Airline_Delays.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E_KWRQAqQ5hqjjTYEgIt8u1jpz1fmcq1

# Big Data Analytics Final Project Airlines Dataset
Names: Mark Stiles, Kelsey Kirby, Gianni Conde

# **Libraries**
"""

! pip install pyspark -q

import io
import pandas as pd
import numpy as np
import seaborn as sns
import time
from timeit import default_timer as timer
from pyspark.sql.functions import col,isnan,when,count
from datetime import timedelta
from string import Formatter
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql import functions as fn
from pyspark.sql.types import StringType, FloatType, StructType, IntegerType
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.ml.classification import GBTClassifier
from pyspark.ml.classification import NaiveBayes
from pyspark.ml.classification import LinearSVC
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.feature import PCA
from pyspark.ml.feature import StandardScaler
from pyspark.ml.feature import MinMaxScaler
from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import Normalizer
from pyspark.ml.feature import OneHotEncoder
from pyspark.ml import Pipeline
#import pyspark.pandas as ps
#from pyspark.sql.types import StringType
#from pyspark.sql.functions import udf
import pyspark.sql.functions as f
from pyspark.sql import types
import numpy as np

"""# **Global Functions**"""

# Print the timestamp now compared to a start time and leave a message
def PrintTimestamp(start_time, message):
  now_time = timer()
  tdelta = timedelta(seconds=now_time-start_time)
  remainder = int(tdelta.total_seconds())

  fmt='{D:02}d {H:02}h {M:02}m {S:02}s'
  f = Formatter()
  desired_fields = [field_tuple[1] for field_tuple in f.parse(fmt)]

  possible_fields = ('W', 'D', 'H', 'M', 'S')
  constants = {'W': 604800, 'D': 86400, 'H': 3600, 'M': 60, 'S': 1}
  values = {}
  for field in possible_fields:
    if field in desired_fields and field in constants:
      values[field], remainder = divmod(remainder, constants[field])

  print(f'{f.format(fmt, **values)}: {message}')

def check_for_nulls(dataframe):
  null_counts = dataframe.select([count(when(col(c).contains('None') | \
            col(c).contains('NULL') | \
            (col(c) == '' ) | \
            col(c).isNull() | \
            isnan(c), c
            )).alias(c)
    for c in airlines_df.columns])
  null_counts.show()

"""# **Context**"""

sc = SparkContext.getOrCreate()

spark = SparkSession\
    .builder\
    .appName('Final Project: Airline Delay Data')\
    .getOrCreate()

"""# **Data Retrieval**

Sources:


*   https://www.kaggle.com/datasets/ulrikthygepedersen/airlines-delay
*   https://www.kaggle.com/datasets/aravindram11/list-of-us-airports
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# if [[ ! -f airline_delay.csv ]]; then
#  wget https://github.com/markstiles/us-airline-data/raw/main/airlines_delay.csv -q
# fi

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# if [[ ! -f airports.csv ]]; then
#  wget https://github.com/markstiles/us-airline-data/raw/main/airports.csv -q
# fi

airline_schema = (StructType()
  .add('Flight', FloatType(), True)
  .add('Time', FloatType(), True)
  .add('Length', FloatType(), True)
  .add('Airline', StringType(), True)
  .add('AirportFrom', StringType(), True)
  .add('AirportTo', StringType(), True)
  .add('DayOfWeek', IntegerType(), True)
  .add('Class', IntegerType(), True)
)
airlines_df = spark.read.csv('airlines_delay.csv', header = True, schema=airline_schema)

def sum_col(num_one, num_two):
    return num_one + num_two

sum_col_udf = f.udf(sum_col, IntegerType())

airlines_df = airlines_df\
  .withColumn('Flight', airlines_df['Flight'].cast(IntegerType()))\
  .withColumn('Time', airlines_df['Time'].cast(IntegerType()))\
  .withColumn('Length', airlines_df['Length'].cast(IntegerType()))\
  .withColumn('ArrivalMin', sum_col_udf(col('Time'), col('Length')).cast(IntegerType()))\
  .withColumnRenamed('Time','DepartureMin')\
  .withColumnRenamed('Length','FlightMin')\
  .withColumnRenamed('AirportFrom','DepartureCode')\
  .withColumnRenamed('AirportTo','ArrivalCode')

print('AIRLINES')
print(f'shape({airlines_df.count()}, {len(airlines_df.columns)})')
airlines_df.printSchema()
airlines_df.show(5)

airport_schema = (StructType()
          .add('IATA', StringType(), True)
          .add('Airport', StringType(), True)
          .add('City', StringType(), True)
          .add('State', StringType(), True)
          .add('Country', StringType(), True)
          .add('Latitude', FloatType(), True)
          .add('Longitude', FloatType(), True)
        )

def plus_180(data_value):
    return data_value + float(180)

plus_180_udf = f.udf(plus_180, FloatType())

airports_df = spark.read.csv('airports.csv', header = True, schema=airport_schema) \
  .withColumn('Longitude', plus_180_udf(col('Longitude')).cast(FloatType())) \
  .withColumnRenamed("IATA", "AirportCode") \
  .drop(col("CITY")) \
  .drop(col("COUNTRY"))

print('AIRPORTS')
print(f'shape({airports_df.count()}, {len(airports_df.columns)})')
airports_df.printSchema()
airports_df.show(5)

check_for_nulls(airlines_df)

"""# **Feature Engineering**

## Day
"""

# function to convert DayOfWeek from integers to days
def convert_days(day_num):
    day_of_week = {1:'Monday', 2:'Tuesday', 3:'Wednesday', 4:'Thursday', 5:'Friday',6:'Saturday',7:'Sunday'}
    return day_of_week.get(day_num)

convert_days_udf = f.udf(convert_days, types.StringType())
airlines_df = airlines_df.withColumn('Day', convert_days_udf(col('DayOfWeek')))

"""## Time of Day"""

# creating column representing Morning, Afternoon, Evening, Night
def time_of_day(minutes):
    # converting minutes elapsed
    time_new = timedelta(minutes=minutes)
    # pulling hours from new time data
    hour = time_new.seconds // 3600

    if 5 <= hour < 12:
                return 'Morning'
    elif 12 <= hour < 17:
                return 'Afternoon'
    elif 17 <= hour < 21:
                return 'Evening'
    else:
        return 'Night'

time_of_day_udf = f.udf(time_of_day, types.StringType())
airlines_df = airlines_df.withColumn('TimeOfDay', time_of_day_udf(col('DepartureMin')))

"""## Departure Time"""

# convert DepartureMin columns format from minutes elapsed to HH:MM
def convert_dep_time(time_value):
  return pd.to_datetime(time_value, unit='m').strftime('%H:%M')

convert_dep_time_udf = f.udf(convert_dep_time, types.StringType())
airlines_df = airlines_df.withColumn('DepartureTime', convert_dep_time_udf(col('DepartureMin')))

"""## Arrival Time"""

# convert ArrivalMin columns format from minutes elapsed to HH:MM
def convert_arr_time(time_value):
  return pd.to_datetime(time_value, unit='m').strftime('%H:%M')

convert_arr_time_udf = f.udf(convert_arr_time, types.StringType())
airlines_df = airlines_df.withColumn('ArrivalTime', convert_arr_time_udf(col('ArrivalMin')))

"""## Airline"""

print('Unique airlines:')
al_list = airlines_df.select("Airline").distinct().collect()
unique_al_list = [al['Airline'] for al in al_list]
print(unique_al_list)

# convert Airline abbreviations with their true names
def convert_airline(airline_code):
    airline_map = {'DL':'Delta Airlines','OO':'SkyWest Airlines','B6':'JetBlue','US':'US Airways',
               'FL':'AirTran Airways','WN':'Southwest Airlines','CO':'Continental Airlines',
               'AA':'American Airlines','YV':'Mesa','EV':'ExpressJet Airlines','XE':'JetSuiteX',
               '9E':'Endeavor Air','OH':'PSA Airlines','UA':'United Airlines','MQ':'Envoy Air',
               'AS':'Alaska Airlines','F9':'Frontier Airlines','HA':'Hawaiian Airlines'}
    return airline_map.get(airline_code)

convert_airline_udf = f.udf(convert_airline, types.StringType())
airlines_df = airlines_df.withColumn('Airline', convert_airline_udf(col('Airline')))

"""## Departure and Arrival"""

# unique values from both Departure and Arrival
dep_list = airlines_df.select('DepartureCode').distinct().collect()
unique_dep_list = [dep['DepartureCode'] for dep in dep_list]

arr_list = airlines_df.select('ArrivalCode').distinct().collect()
unique_arr_list = [arr['ArrivalCode'] for arr in arr_list]

both = np.unique(np.concatenate((unique_dep_list, unique_arr_list)))
print('Unique airport codes:')
print(both)

airlines_df = airlines_df \
  .join(airports_df, airlines_df.DepartureCode == airports_df.AirportCode, 'inner') \
  .drop('AirportCode') \
  .withColumnRenamed('Airport', 'DepartureAirport') \
  .withColumnRenamed('State', 'DepartureState') \
  .withColumnRenamed('Latitude', 'DepartureLatitude') \
  .withColumnRenamed('Longitude', 'DepartureLongitude') \

airlines_df = airlines_df \
  .join(airports_df, airlines_df.ArrivalCode == airports_df.AirportCode, 'inner') \
  .drop('AirportCode') \
  .withColumnRenamed('Airport', 'ArrivalAirport') \
  .withColumnRenamed('State', 'ArrivalState') \
  .withColumnRenamed('Latitude', 'ArrivalLatitude') \
  .withColumnRenamed('Longitude', 'ArrivalLongitude')

"""## Flight Duration"""

# convert FlightMin columns format from minutes elapsed to HH:MM
def convert_flight_length(length_value):
  return pd.to_datetime(length_value, unit='m').strftime('%H:%M')

convert_length_udf = f.udf(convert_flight_length, types.StringType())
airlines_df = airlines_df.withColumn('FlightDuration', convert_length_udf(col('FlightMin')))

"""## Final Schema"""

print('AIRLINES')
print(f'shape({airlines_df.count()}, {len(airlines_df.columns)})')
airlines_df.printSchema()
airlines_df.show(5)

"""# **Null Checks**"""

check_for_nulls(airlines_df)

"""# **Airlines Dataframe Import**
This is the exported dataframe after cleaning and engineering used to prevent having to recalculate the dataset each time the runtime was changed between CPU and GPU

## Import
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# if [[ ! -f airlines_df.csv ]]; then
#  wget https://github.com/markstiles/us-airline-data/raw/main/airlines_df.csv -q
# fi

import_schema = (StructType()
	.add('Flight', IntegerType(), True)
	.add('DepartureMin', IntegerType(), True)
	.add('FlightMin', IntegerType(), True)
	.add('Airline', StringType(), True)
	.add('DepartureCode', StringType(), True)
	.add('ArrivalCode', StringType(), True)
	.add('DayOfWeek', IntegerType(), True)
	.add('Class', IntegerType(), True)
	.add('ArrivalMin', IntegerType(), True)
	.add('Day', StringType(), True)
	.add('TimeOfDay', StringType(), True)
	.add('DepartureTime', StringType(), True)
	.add('FlightDuration', StringType(), True)
	.add('ArrivalTime', StringType(), True)
	.add('DepartureAirport', StringType(), True)
	.add('DepartureState', StringType(), True)
	.add('DepartureLatitude', FloatType(), True)
	.add('DepartureLongitude', FloatType(), True)
	.add('ArrivalAirport', StringType(), True)
	.add('ArrivalState', StringType(), True)
	.add('ArrivalLatitude', FloatType(), True)
	.add('ArrivalLongitude', FloatType(), True)
)

airlines_df = spark.read.csv('airlines_df.csv', header = True, schema=import_schema)

"""# **Exploratory Data Analysis**

## Distributions
"""

# numeric distributions
num_arr = ['Class', 'DepartureLongitude', 'DepartureLatitude', 'DepartureMin', 'ArrivalLongitude', 'ArrivalLatitude', 'ArrivalMin', 'DayOfWeek', 'FlightMin', 'Flight']
for numeric_category in num_arr:
  airlines_df.select(numeric_category).toPandas()\
    .plot(kind = 'hist', title='', xlabel='', ylabel='', bins=20, rwidth=0.7);

"""## Enumeration Barplots"""

too_many = ['ArrivalAirport', 'DepartureAirport', 'ArrivalTime', 'FlightDuration', 'DepartureTime']
cat_arr = ['Airline', 'Day', 'TimeOfDay', 'ArrivalState', 'DepartureState']
for category in cat_arr:
    count_label = 'Count'
    airlines_df \
      .groupBy(category) \
      .agg(fn.count(category).alias(count_label)) \
      .select(category, count_label) \
      .orderBy(count_label) \
      .toPandas() \
      .plot(kind = 'bar', x=category, y=count_label, title=category, xlabel='', ylabel='')

"""## Pairs Plot"""

pair_time = timer()

pair_arr = ['DepartureLongitude', 'DepartureLatitude', 'DepartureMin', 'ArrivalLongitude', 'ArrivalLatitude', 'ArrivalMin', 'DayOfWeek', 'FlightMin']
pair_len = len(pair_arr)

fig, ax = plt.subplots(pair_len,pair_len)
fig.set_figwidth(20)
fig.set_figheight(12)
fig.tight_layout()

pair_row = 0
for col in pair_arr:
  pair_col = 0
  temp_col = airlines_df.select(col).toPandas()[col]

  for col2 in pair_arr:
    PrintTimestamp(pair_time, f'{col} x {col2} - row:{pair_row};col:{pair_col}')
    cur_ax = ax[pair_row,pair_col]
    if(col == col2):
      cur_ax.hist(temp_col, bins=20, rwidth=0.7)
      cur_ax.set_title(col)
    else:
      temp_col2 = airlines_df.select(col2).toPandas()[col2]
      cur_ax.scatter(temp_col, temp_col2, marker=".")
      cur_ax.set_title(f'{col} x {col2}')
    pair_col += 1
  pair_row += 1

PrintTimestamp(pair_time, f'Finished Pairs Plot')

"""## Flight Density Map"""

col = 'DepartureLongitude'
col_label = 'DepLongSum'

col2 = 'DepartureLatitude'
col2_label = 'DepLatSum'

col3 = 'DepartureMin'
col3_label = 'DepMinAvg'

temp_df = airlines_df.select(col, col2, col3).toPandas()
plt.scatter(temp_df[col], temp_df[col2], marker=".", alpha=0.01)

"""# **Principal Component Analysis**"""

pca_time = timer()

# build the model pipe
pca_pipe = Pipeline(stages = [
  VectorAssembler(
    inputCols = ['DepartureMin', 'FlightMin', 'DayOfWeek', 'ArrivalLatitude', 'ArrivalLongitude', 'DepartureLatitude', 'DepartureLongitude'],
    outputCol='features_pre', handleInvalid="skip"),
  StandardScaler(inputCol='features_pre', outputCol='features', withMean = True, withStd = False),
  PCA(k=7, inputCol='features', outputCol='scores')
])

# split train/test data
pca_train_set, pca_test_set = airlines_df.sample(fraction=1.0).limit(10000).randomSplit([0.8, 0.2], seed = 1980)

print(f'*** PCA ***')
print()

print(f'{pca_train_set.count()} observations')

PrintTimestamp(pca_time, f'Train start')

# fit the model
pca_model = pca_pipe.fit(pca_train_set)

PrintTimestamp(pca_time, f'Analysis Start')

explained_var = pca_model.stages[-1].explainedVariance

print(f'Explained Variance: {explained_var}')

pc_values = pca_model.stages[-1].pc.toArray()

print(f'PC: {pc_values}')

PrintTimestamp(pca_time, f'Analysis end')

plt.figure()
plt.plot(np.arange(1, len(explained_var) + 1), explained_var)
plt.title('Scree Plot')
plt.xlabel('Principal Component')
plt.ylabel('Proportion Variance Explained')

cum_sum = np.cumsum(explained_var)
plt.figure()
plt.plot(np.arange(1, len(explained_var)+1), cum_sum)
plt.ylabel('Cumulative Sum of Variance Explained')
plt.xlabel('Cumulative Components')
plt.title('Cumulative Sum of Variance Explained')

# ensure the features aren't correlated

from pyspark.ml.stat import *

# use the numeric columns to measure correlations
num_cols = ['DepartureMin', 'FlightMin', 'DayOfWeek', 'ArrivalLatitude', 'ArrivalLongitude', 'DepartureLatitude', 'DepartureLongitude']
for col_1 in num_cols:
  for col_2 in num_cols:
    if (col_1 == col_2):
      continue

    p_corr = airlines_df.corr(col_1, col_2, 'pearson')
    if abs(p_corr) < 0.5:
      continue

    corr_sign = 'positive' if p_corr >= 0 else 'negative'

    # print the correlation value
    print(f'The pearson correlation between \'{col_1}\' and \'{col_2}\' is strong and {corr_sign} with a magnitude of {p_corr}')

"""# **Pipeline Model**

## Functions
"""

# Calculate and print the 'Area Under Curve'
def PrintAUC(evaluator, prediction_set):
  log_auc = evaluator.evaluate(prediction_set)

  print(f'Test AUC score: {log_auc:.1%}')

# Get the best model hyperparameters and print the details
def PrintModelDetails(cv_model):
  best_model = cv_model.bestModel.stages[-1]
  bestParams = best_model.extractParamMap()
  usedParams = ['maxDepth', 'stepSize', 'maxIter', 'subsamplingRate']

  print('The best model parameters were:')
  for x in bestParams.keys():
    if(x.name in usedParams):
      print(f'    {x.name}: {bestParams[x]}')

# Calculate and print the 'Accuracy'
def PrintAccuracy(prediction_set):
  predicted_values = [row['prediction'] for row in prediction_set.collect()]
  actual_values = [row['Class'] for row in prediction_set.collect()]
  correct_predictions = [int(pred == true) for pred, true in zip(predicted_values, actual_values)]

  print(f'Accuracy: {(sum(correct_predictions) / len(correct_predictions)):.0%}')

"""## Model Pipeline"""

def TrainModel(algorithm, param_grid):

  start_time = timer()

  # create stage list
  stages = [
    StringIndexer(
      inputCols=['Airline', 'TimeOfDay', 'DepartureState'],
      outputCols=['Airline_indexed', 'TimeOfDay_indexed', 'DepartureState_indexed'],
      handleInvalid='skip'),
    OneHotEncoder(
      inputCols=['Airline_indexed', 'TimeOfDay_indexed', 'DayOfWeek'],
      outputCols=['Airline_ohe', 'TimeOfDay_ohe', 'DayOfWeek_ohe']),
    VectorAssembler(
      inputCols = ['DepartureMin', 'FlightMin', 'Airline_ohe'],
      outputCol='features_assembled'),
    StandardScaler(inputCol='features_assembled', outputCol='features')
  ]

  # join default and custom stages
  all_stages = stages + [algorithm]

  # build the model pipe
  pipe = Pipeline(stages = all_stages)

  # split train/test data
  #train_set, test_set = airlines_df.sample(fraction=1.0).limit(100000).randomSplit([0.8, 0.2], seed = 1980)
  train_set, test_set = airlines_df.randomSplit([0.8, 0.2], seed = 1980)

  print(f'*** {type(algorithm).__name__} ***')
  print()

  print(f'{train_set.count()} observations')

  PrintTimestamp(start_time, f'Buid Pipeline and Cross Validator')

  # crossvalidator
  eval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='Class')
  cv = CrossValidator(estimator=pipe, estimatorParamMaps=param_grid, evaluator=eval, parallelism=2, numFolds=3)

  PrintTimestamp(start_time, f'Train start')

  # fit the model
  model = cv.fit(train_set)

  PrintTimestamp(start_time, f'Train end')

  # test the test data
  predictions = model.transform(test_set)

  PrintTimestamp(start_time, f'Predict end')

  # score results
  print()
  PrintAUC(eval, predictions)
  PrintAccuracy(predictions)

  # parameter results
  print()
  PrintModelDetails(model)

"""## Logistic Regression Classifier


"""

lr = LogisticRegression(featuresCol='features', labelCol='Class', family="binomial")
grid = ParamGridBuilder() \
  .addGrid(lr.maxIter, [5, 10, 15]) \
  .build()

TrainModel(lr, grid)

"""## Naive Bayes




"""

nb = NaiveBayes(featuresCol='features', labelCol='Class')
grid = ParamGridBuilder() \
  .build()

TrainModel(nb, grid)

"""## Random Forest"""

rfc = RandomForestClassifier(featuresCol='features', labelCol='Class')

grid = ParamGridBuilder() \
  .addGrid(rfc.maxDepth, [20, 25, 30]) \
  .addGrid(rfc.subsamplingRate, [0.075, 0.1, 0.15]) \
  .build()

TrainModel(rfc, grid)

"""## Gradient Boosted Tree"""

gbt = GBTClassifier(featuresCol='features', labelCol='Class', maxIter=10, maxDepth=4, stepSize=0.05, subsamplingRate=0.1)

grid = ParamGridBuilder() \
  .addGrid(gbt.maxDepth, [4, 5, 6]) \
  .addGrid(gbt.maxIter, [15, 20, 25]) \
  .addGrid(gbt.stepSize, [0.1, 0.2, 0.3]) \
  .build()

TrainModel(gbt, grid)

"""## Decision Tree Classifier"""

dt = DecisionTreeClassifier(featuresCol='features', labelCol='Class')

grid = ParamGridBuilder() \
  .addGrid(dt.maxDepth, [20, 25, 30]) \
  .build()

TrainModel(dt, grid)

"""## Support Vector Machine"""

svm = LinearSVC(featuresCol='features', labelCol='Class')

grid = ParamGridBuilder() \
  .addGrid(svm.maxIter, [1, 2, 3]) \
  .build()

TrainModel(svm, grid)